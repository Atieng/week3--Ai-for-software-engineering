{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethics_troubleshooting.ipynb\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: ETHICAL CONSIDERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff8701",
   "metadata": {},
   "source": [
    "### Ethical Analysis of the MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da956bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data to analyze potential biases\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"MNIST Dataset Analysis for Bias Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze label distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "label_distribution = dict(zip(unique, counts))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(label_distribution.keys(), label_distribution.values())\n",
    "plt.title('MNIST Training Set Label Distribution')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Image shape: {X_train[0].shape}\")\n",
    "\n",
    "# Analyze potential biases\n",
    "print(\"\\nüîç POTENTIAL BIASES IN MNIST DATASET:\")\n",
    "\n",
    "# 1. Cultural/Regional Bias\n",
    "print(\"1. Cultural/Regional Bias:\")\n",
    "print(\"   - Digits written predominantly by North American writers\")\n",
    "print(\"   - Underrepresentation of Asian, European handwriting styles\")\n",
    "print(\"   - Cultural variations in digit formation (e.g., '1' with base, '7' with crossbar)\")\n",
    "\n",
    "# 2. Demographic Bias\n",
    "print(\"\\n2. Demographic Bias:\")\n",
    "print(\"   - Primarily collected from Census Bureau employees and high school students\")\n",
    "print(\"   - Age bias: underrepresents elderly and very young handwriting\")\n",
    "print(\"   - Educational bias: mostly educated individuals\")\n",
    "\n",
    "# 3. Data Quality Bias\n",
    "print(\"\\n3. Data Quality Bias:\")\n",
    "print(\"   - Clean, centered digits don't represent real-world messy handwriting\")\n",
    "print(\"   - Limited variation in writing instruments and paper quality\")\n",
    "print(\"   - No representation of degraded or low-quality images\")\n",
    "\n",
    "# 4. Representation Bias\n",
    "print(\"\\n4. Representation Bias:\")\n",
    "print(\"   - Equal distribution enforced artificially\")\n",
    "print(\"   - Real-world digit frequency not reflected (e.g., more 0s and 1s in some contexts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dd0d8",
   "metadata": {},
   "source": [
    "### Ethical Analysis of Amazon Reviews Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a34d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Amazon reviews analysis for bias detection\n",
    "print(\"\\nüîç POTENTIAL BIASES IN AMAZON REVIEWS MODEL:\")\n",
    "\n",
    "# 1. Language and Cultural Bias\n",
    "print(\"1. Language and Cultural Bias:\")\n",
    "print(\"   - Primarily English-language reviews\")\n",
    "print(\"   - Western cultural perspectives dominate\")\n",
    "print(\"   - Non-native English speakers might be misclassified\")\n",
    "\n",
    "# 2. Product Category Bias\n",
    "print(\"\\n2. Product Category Bias:\")\n",
    "print(\"   - Electronics overrepresented in training data\")\n",
    "print(\"   - Luxury vs. budget product sentiment differences\")\n",
    "print(\"   - Brand popularity affecting review volume and sentiment\")\n",
    "\n",
    "# 3. Demographic Bias\n",
    "print(\"\\n3. Demographic Bias:\")\n",
    "print(\"   - Younger, tech-savvy users overrepresented\")\n",
    "print(\"   - Geographic bias (US-centric reviews)\")\n",
    "print(\"   - Income level bias in product reviews\")\n",
    "\n",
    "# 4. Sentiment Analysis Bias\n",
    "print(\"\\n4. Sentiment Analysis Bias:\")\n",
    "print(\"   - Sarcasm and irony often misclassified\")\n",
    "print(\"   - Cultural differences in expression of satisfaction\")\n",
    "print(\"   - Positive bias (people more likely to review extreme experiences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d1238",
   "metadata": {},
   "source": [
    "### Bias Mitigation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüõ°Ô∏è BIAS MITIGATION STRATEGIES\")\n",
    "\n",
    "print(\"For MNIST Model:\")\n",
    "print(\"1. Data Augmentation:\")\n",
    "print(\"   - Apply rotations, skews, and noise to simulate diverse handwriting\")\n",
    "print(\"   - Include datasets from other regions (e.g., Chinese MNIST)\")\n",
    "print(\"   - Synthetic data generation for underrepresented styles\")\n",
    "\n",
    "print(\"\\n2. TensorFlow Fairness Indicators:\")\n",
    "print(\"   - Analyze performance across synthetic demographic groups\")\n",
    "print(\"   - Monitor accuracy disparities between digit classes\")\n",
    "print(\"   - Implement fairness constraints during training\")\n",
    "\n",
    "print(\"\\n3. Evaluation Metrics:\")\n",
    "print(\"   - Use equalized odds and demographic parity\")\n",
    "print(\"   - Analyze confusion matrices per 'style' groups\")\n",
    "print(\"   - Cross-validation with stratified sampling\")\n",
    "\n",
    "print(\"\\nFor Amazon Reviews Model:\")\n",
    "print(\"1. spaCy's Rule-Based Systems:\")\n",
    "print(\"   - Create custom rules for cultural expressions\")\n",
    "print(\"   - Handle sarcasm and irony with pattern matching\")\n",
    "print(\"   - Domain-specific sentiment dictionaries\")\n",
    "\n",
    "print(\"\\n2. Data Diversification:\")\n",
    "print(\"   - Collect reviews from multiple regions and languages\")\n",
    "print(\"   - Balance product categories and price ranges\")\n",
    "print(\"   - Include diverse demographic information\")\n",
    "\n",
    "print(\"\\n3. Fairness Monitoring:\")\n",
    "print(\"   - Regular bias audits\")\n",
    "print(\"   - A/B testing with different demographic groups\")\n",
    "print(\"   - Continuous model monitoring in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad697973",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
